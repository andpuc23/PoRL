{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e839c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from envs.test_env import Electric_Car\n",
    "\n",
    "class TabularQLearning():\n",
    "    def __init__(self, data_path, discount_rate = 0.95, bin_size = 100):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        discount_rate = discount rate used for future rewards\n",
    "        bin_size = number of bins used for discretizing the state space\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.discount_rate = discount_rate\n",
    "        self.bin_size = bin_size\n",
    "        self.env = Electric_Car(path_to_test_data=data_path)\n",
    "        self.action_space = self.env.continuous_action_space\n",
    "        \n",
    "        self.low = self.env.observation_space.low\n",
    "        self.high = self.env.observation_space.high\n",
    "    \n",
    "        self.bins_battery = np.linspace(self.low[0], self.high[0], self.bin_size) \n",
    "        self.bins_price = np.linspace(self.low[1], self.high[1], self.bin_size) \n",
    "        \n",
    "        self.bins = [self.bins_battery, self.bins_price]\n",
    "    \n",
    "    def discretize_state(self, state, cont_features = [0,1]):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        state = state observation that needs to be discretized\n",
    "        \n",
    "        Returns:\n",
    "        discretized state\n",
    "        '''\n",
    "        for feature in cont_features:\n",
    "        \n",
    "        self.state = state\n",
    "        digitized_state = []\n",
    "        \n",
    "        for i in range(len(self.bins)):\n",
    "            digitized_state.append(np.digitize(self.state[i], self.bins[i])-1)\n",
    "        \n",
    "        \n",
    "        return digitized_state\n",
    "    \n",
    "    def create_Q_table(self):\n",
    "        '''\n",
    "        Returns:\n",
    "        Q-table with zeros\n",
    "        '''\n",
    "        \n",
    "        self.state_space = self.bin_size - 1\n",
    "        self.Qtable = np.zeros((self.state_space, self.state_space, self.action_space))\n",
    "        \n",
    "\n",
    "    def train(self, simulations, learning_rate, epsilon = 0.05, epsilon_decay = 1000, adaptive_epsilon = False, \n",
    "              adapting_learning_rate = False):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        \n",
    "        simulations = number of episodes of a game to run\n",
    "        learning_rate = learning rate for the update equation\n",
    "        epsilon = epsilon value for epsilon-greedy algorithm\n",
    "        epsilon_decay = number of full episodes (games) over which the epsilon value will decay to its final value\n",
    "        adaptive_epsilon = boolean that indicates if the epsilon rate will decay over time or not\n",
    "        adapting_learning_rate = boolean that indicates if the learning rate should be adaptive or not\n",
    "        \n",
    "        '''\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon_start = 1\n",
    "        self.epsilon_end = 0.05\n",
    "\n",
    "        self.rewards = []\n",
    "        self.average_rewards = []\n",
    "        self.create_Q_table()\n",
    "        \n",
    "        if adapting_learning_rate:\n",
    "            self.learning_rate = 1\n",
    "        \n",
    "        for i in range(simulations):\n",
    "            #if i % 5000 == 0:\n",
    "            #    print(f'Please wait, the algorithm is learning! The current simulation is {i}')\n",
    "            \n",
    "            done = False\n",
    "            \n",
    "            state = self.env.reset()[0]\n",
    "            state = self.discretize_state(state)\n",
    "            \n",
    "            total_rewards = 0\n",
    "            \n",
    "            if adaptive_epsilon:\n",
    "                self.epsilon = np.interp(i, [0, self.epsilon_decay], [self.epsilon_start, self.epsilon_end])\n",
    "\n",
    "            while not done:\n",
    "\n",
    "                if np.random.uniform(0,1) > 1-self.epsilon:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(self.Qtable[state[0],state[1],:])\n",
    "                    \n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "                done =  terminated or truncated\n",
    "                \n",
    "                next_state = self.discretize_state(next_state)\n",
    "\n",
    "                Q_target = (reward + self.discount_rate*np.max(self.Qtable[next_state[0], next_state[1]]))\n",
    "                delta = self.learning_rate * (Q_target - self.Qtable[state[0], state[1], action])\n",
    "                self.Qtable[state[0], state[1], action] = self.Qtable[state[0], state[1], action] + delta\n",
    "                \n",
    "                total_rewards += reward\n",
    "                state = next_state      \n",
    "            \n",
    "            if adapting_learning_rate:\n",
    "                self.learning_rate = self.learning_rate/np.sqrt(i+1)\n",
    "            \n",
    "            self.rewards.append(total_rewards)\n",
    "            \n",
    "            #Calculate the average score over 100 episodes\n",
    "            if i % 100 == 0:\n",
    "                self.average_rewards.append(np.mean(self.rewards))\n",
    "                \n",
    "                #Initialize a new reward list, as otherwise the average values would reflect all rewards!\n",
    "                self.rewards = []\n",
    "        \n",
    "        print('The simulation is done!')\n",
    "        \n",
    "    def visualize_rewards(self):\n",
    "        pass\n",
    "            \n",
    "    def play_game(self):\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
