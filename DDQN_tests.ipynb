{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.environment import Environment\n",
    "from envs.test_env import Electric_Car\n",
    "from models.baseline import BaselineModel\n",
    "from models.DQN import DQNModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "from envs.test_env import Electric_Car\n",
    "from models.DQN import DQNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = Electric_Car('data/train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNModel(test_env, replay_capasity=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.500e+01, 2.431e+01, 1.000e+00, 0.000e+00, 1.000e+00, 1.000e+00,\n",
       "       2.007e+03, 1.000e+00])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_env.observation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env stop\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1096 is out of bounds for axis 0 with size 1096",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m27000\u001b[39m):\n\u001b[0;32m      3\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     state, reward, term, trunc, act \u001b[38;5;241m=\u001b[39m \u001b[43mtest_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mindex)] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39mstate, act, reward]\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m term \u001b[38;5;129;01mor\u001b[39;00m trunc:\n",
      "File \u001b[1;32mc:\\Users\\snipercapt\\Desktop\\PoRL\\envs\\test_env.py:107\u001b[0m, in \u001b[0;36mElectric_Car.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    103\u001b[0m truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    105\u001b[0m info \u001b[38;5;241m=\u001b[39m action  \u001b[38;5;66;03m# The final action taken after all constraints! For debugging purposes\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Update the state\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\snipercapt\\Desktop\\PoRL\\envs\\test_env.py:113\u001b[0m, in \u001b[0;36mElectric_Car.observation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobservation\u001b[39m(\u001b[38;5;28mself\u001b[39m):  \u001b[38;5;66;03m# Returns the current state\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     battery_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbattery_level\n\u001b[1;32m--> 113\u001b[0m     price \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprice_values\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mday\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhour \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    114\u001b[0m     hour \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhour\n\u001b[0;32m    115\u001b[0m     day_of_week \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mday \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdayofweek  \u001b[38;5;66;03m# Monday = 0, Sunday = 6\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1096 is out of bounds for axis 0 with size 1096"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['battery_level', 'price', 'hour', 'day_of_week', 'day_of_year', 'month', 'year', 'available', 'action', 'reward'])\n",
    "for row in range(27000):\n",
    "    action = np.random.random(1)\n",
    "    state, reward, term, trunc, act = test_env.step(action)\n",
    "    df.loc[len(df.index)] = [*state, act, reward]\n",
    "    if term or trunc:\n",
    "        print('env stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26303, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reward.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.columns[:-1]]\n",
    "y = df.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env = Electric_Car('data/validate.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_action_forest(env):\n",
    "    max_action = -2\n",
    "    max_reward = float('-inf')\n",
    "    obs = env.observation()\n",
    "    for action in np.linspace(-1, 1, 21):\n",
    "        pred = forest.predict([[*obs, action]])\n",
    "        if pred > max_reward:\n",
    "            max_reward = pred\n",
    "            max_action = action\n",
    "    return max_action, max_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17519/17519 [00:00<00:00, 17562.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "rewards = []\n",
    "for i in tqdm(range(17519)):\n",
    "    # action, reward = pick_action_forest(val_env)\n",
    "    action = (np.random.random()-0.5)*2\n",
    "    obs, _, tr, te, _ = val_env.step(action)\n",
    "    rewards.append(reward)\n",
    "    done = tr or te\n",
    "\n",
    "print(sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Electric_Car('data/train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQNModel(env, replay_capasity=10, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 39.58it/s]00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Action must be between -1 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\snipercapt\\Desktop\\PoRL\\models\\DQN.py:146\u001b[0m, in \u001b[0;36mDQNModel.train\u001b[1;34m(self, num_episodes)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tqdm(count()):\n\u001b[0;32m    145\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__select_action(state)\n\u001b[1;32m--> 146\u001b[0m     observation, reward, truncated, terminated, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m     done \u001b[38;5;241m=\u001b[39m truncated \u001b[38;5;129;01mor\u001b[39;00m terminated\n\u001b[0;32m    148\u001b[0m     reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([reward], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\snipercapt\\Desktop\\PoRL\\envs\\test_env.py:36\u001b[0m, in \u001b[0;36mElectric_Car.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     33\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(action)  \u001b[38;5;66;03m# Remove the extra dimension\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m action \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAction must be between -1 and 1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Calculate if, at 7am and after the chosen action, the battery level will be below the minimum morning level:\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhour \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m7\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Action must be between -1 and 1"
     ]
    }
   ],
   "source": [
    "model.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait, the experience replay buffer will be filled with random transitions\n",
      "Initialization with random transitions is done!\n"
     ]
    }
   ],
   "source": [
    "from models.DeepQLearning import DDQNAgent\n",
    "test_env = Electric_Car('data/validate.xlsx')\n",
    "model = DDQNAgent('data/validate.xlsx', device='cpu', epsilon_decay=10_000, epsilon_start=1.0,\n",
    "                   epsilon_end=.05, discount_rate=.99, lr=5e-4, buffer_size=50_000)\n",
    "model.init_envs(test_env)\n",
    "model.load('ddqn_tn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load('ddqn_tn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
